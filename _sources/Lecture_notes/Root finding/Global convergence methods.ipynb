{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mwelland/ENGPYHS_3NM4/blob/main/Root_finding_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pn-VhfqZjo6"
   },
   "source": [
    "Goals:\n",
    "- See the pitfalls of Newton's method\n",
    "- Understand the ND Newton-Raphson method\n",
    "- Awareness of linesearch algorithms for improved convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvDpSNJys_30"
   },
   "source": [
    "# 'Global' convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3a2LKejRBeH"
   },
   "source": [
    "There are several options to modify the Newton-Raphson method in order to enhance the robustness of root finding, but the improvement in robustness has to be weighed against the computational expense.\n",
    "\n",
    "\n",
    "\n",
    "We **have** to assume our initial guess is reasonable, so the goal is to ensure the solution doesn't *wander*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4EU-lG6Wy79"
   },
   "source": [
    "## Line search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bh4G2YJtXpUu"
   },
   "source": [
    "*If* we trust that $\\Delta \\vec{x}$ is pointing in the right direction, then we should make sure it doesn't *step too far*.\n",
    "\n",
    "These approaches are called *line search* alrogithms since we are moving along the direction prescribed by $\\Delta \\vec{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2Wo7ODRRlsa"
   },
   "source": [
    "### Damped Newton-Raphson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPBi4qiTRrVn"
   },
   "source": [
    "The easiest modification is adding a damping term. Calculate $\\Delta \\vec{x}$ as usual from $J \\Delta \\vec{x} = - \\vec{f}$ but update the step a scalar factor of the increment:\n",
    "$$\n",
    "\\vec{x}^{i+1} = \\vec{x}^i+\\alpha \\Delta \\vec{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxiwwU8RUoP2"
   },
   "source": [
    "For $\\alpha =1$ we recover the method.\n",
    "\n",
    "For $0 \\lt \\alpha  \\lt 1$ the method is underdamped, and the solver forced to take small steps, keeping it closer to the guess but at the cost of convergence speed.\n",
    "\n",
    "For $\\alpha  \\gt 1$ the method is overdamped, and solver steps exagerated. This may seem like a good idea to speed things up but if you are not careful you will constantly overshoot your root or produce frenetic behaviour!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zyw2mhYHXvJh"
   },
   "source": [
    "### Optimal step size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJ8y6tRHUPyO"
   },
   "source": [
    "From the *damped* approach, we can also attempt to find a 'good' step size algorithmically. Near the root, we know we want $\\alpha=1$ to get quadratic convergence, so this is often a starting point.\n",
    "\n",
    "There are several approaches which levarage the information we have:\n",
    "* $\\vec{f}(\\vec{x})$\n",
    "* $J(\\vec{x})$\n",
    "* $\\vec{f}(\\vec{x}+\\Delta \\vec{x})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV2CgTOiae3X"
   },
   "source": [
    "### Fit a quadratic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2U3I9ncsWdS"
   },
   "source": [
    "With 3 pieces of information, we can fit a quadratic in the search direction and choose the minimum for the update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXXmOLsobHbo"
   },
   "source": [
    "### Backtracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IBfuxQxbJfR"
   },
   "source": [
    "The backtracking routine starts with $\\alpha=1$ and then subdivides it until\n",
    "\n",
    "$$\\vec{f}(\\vec{x}+\\alpha \\Delta \\vec{x}) - \\vec{f}(\\vec{x}) \\approx J(\\vec{x}) \\alpha \\Delta \\vec{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AihdikaMse4w"
   },
   "source": [
    "### Bounded minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NgWiN9-spi7"
   },
   "source": [
    "Find $\\alpha$ in the range (0,1] that minimizes some measure of residual, e.g. $\\| \\vec{f}(\\vec{x}+\\alpha \\Delta \\vec{x}) \\|$.\n",
    "\n",
    "This is a 1D minimizaqtion problem (similar to our 1D root finding) and can use similar tools e.g.: Secant method. The tradeoff here is the efficiency of this minimzation vs just taking another Newton step."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNQAgPexNuV2ICYVWd0fdOV",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
