{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "it4HJ8K9ADIT"
   },
   "source": [
    "# Indefinite systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlZjhbwKAz7l"
   },
   "source": [
    "The quadratic $x^2 - 6 x y +y^2$ is an example of a problem with an *indefinite* matrix. Such functions are neither convex nor concave, and the *extremal point* is neither a minimum nor a maximum, but a *saddle point* (as visualized).\n",
    "\n",
    "Saddle point problems are an important subclass of optimization as we'll see when we deal with constraints.  \n",
    "\n",
    "Newton's method, and generally optimizers that employ gradients, typically can identify saddle points, but direct methods often fail unless one modifies the objective function (e.g.: Minimize $f^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6rcXbGjEp97"
   },
   "source": [
    "## Eigenvalues / functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUxSQeDbEuPh"
   },
   "source": [
    "The fact that $f(x,y) = x^2 - 6 x y +y^2$ is indefinite may come as a surprise considering the quadratics along the axes, $f(x,0) = x^2$ and $f(0,y) = y^2$ are obviously positive definite.\n",
    "\n",
    "The key is to realize that we could rotate the coordinate system by $45^0$ (which obviously wouldn't change the extremal), and write the same function as $f(x',y') = 8 {x'}^2 - 4 {y'}^2$ from which the saddle point is clear.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGCaw3TXIQwz"
   },
   "source": [
    "*But how would you know that?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1swjKZ3IOPw"
   },
   "source": [
    "The $x',y'$ coordiate system is *special* in that its Hessian doesn't have diagonal terms. They are the *eigenvectors* of the Hessian, and the $8$ and $-4$ are the *eigenvalues*. With this in mind, definiteness is defined:\n",
    "\n",
    "| Definiteness | Eigenvalues |\n",
    "|---|---|\n",
    "| Positive Definite | All eigenvalues are positive. |\n",
    "| Negative Definite | All eigenvalues are negative. |\n",
    "| Indefinite | Eigenvalues are positive and negative. |\n",
    "| Positive Semi-Definite | All eigenvalues are non-negative (positive or zero). |\n",
    "| Negative Semi-Definite | All eigenvalues are non-positive (negative or zero). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5RJ-w7qFiDn",
    "outputId": "ea199ef4-c2b6-4421-b004-fcbc7788868d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [ 8. -4.]\n",
      "Eigenvectors: [[ 0.70710678  0.70710678]\n",
      " [-0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "# prompt: what are the eigen functions and values of x^2 - 6xy + y^2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def hessian_f(x):\n",
    "  return np.array([[2, -6], [-6, 2]])\n",
    "\n",
    "hessian = hessian_f([0,0])\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(hessian)\n",
    "\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors:\", eigenvectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxJLcuxI4IPu"
   },
   "source": [
    "Newton's optimization routine is highly sensative to initial guesses. Furthermore, the calculation and inversion of the Hessian can be computationally expensive and we don't necessarily *trust* that it will give us a good result. We will return to it later in its more common form..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMVMLK1siVxZJxIGU0ReZHy",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
