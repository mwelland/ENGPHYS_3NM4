{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guass Elimination / LU decomposition are *direct* solvers in that they solve systems in a fixed number of operations. If we could calculate in infinite precision these answers would be exact. Direct solvers generally scale ~$O(n^3)$ and are therefore impractical for large systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now talk about *iterative / indirect methods* which start with an initial guess and iterate (hopefully) towards a solution. Each iteration is typically faster and more memory efficient than direct solvers, but convergence is only guaranteed under certain limited circumstances. Even then, the *rate of convergence* can still make these techniques impractical.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative techniques have some useful properties:\n",
    "1. They are self-correcting in that roundoff (or arithmatic) errors are corrected through further cycles.\n",
    "2. They are more conducive to sparse matrix storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the solution vector $x$ is altered during these computations. Neither the matrix (nor anything we will do to it) is not altered during any of these steps!\n",
    "\n",
    "> In fact, there are a class of solvers called *matrix free* solvers that avoid the explicit writing of $A$ entirely... This is useful if the matrix product $Ax$ can be calculated in a more effective manner such that we don't bother writing out and storing A$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each iteration will (hopefully) improve the answer asymptotically, so we estimate the error and say when it is stopped getting better. As discussed with error analysis, this involves defining tolerances for computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider: What does it mean for a vector $x$ to satisfy $Ax=b$?\n",
    "\n",
    "-> All the elements in $x$ must be *consistent*.\n",
    "\n",
    "What if one element was not consistent?\n",
    "\n",
    "-> We could solve for it based on the others!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big picture algorithm\n",
    "1. Start with a guess\n",
    "2. For each iterative step, identify a single element in $x$, make it consistent with all the others.\n",
    "3. Check how well the guess solves the linear system\n",
    "4. Repeat (possibly updating a parameter first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system has converged when $Ax=b$. Define the *residual vector*, $R = Ax-b$ such that when $R\\rightarrow 0$ (the system is converged).\n",
    "\n",
    "Let's use the norm (some measure of magnitude) of $R$  and say the system has converged when $||R|| \\lt tolerance$.\n",
    "\n",
    "> Should we use absolute tolerance or relative?\n",
    "\n",
    "> What other tolerance could we imagine using? (Hint: We are actually interested $x$...)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
