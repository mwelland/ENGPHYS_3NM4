{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mwelland/ENGPYHS_3NM4/blob/main/Linear_systems_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwZZirxA8hJv"
   },
   "source": [
    "# LU decomposition\n",
    "\n",
    "Commonly we will have to repeatedly solve\n",
    "$Ax = b$ for multiple $b_i$. Gauss Elimination for each $b_i$ would be grossly inefficient. If you knew all the $b_i$ in advance you could do this in parallel by forming the augmented matrix:\n",
    "\n",
    "$[A|b_1 \\ b_2 \\  b_3 \\ ...]$\n",
    "\n",
    "but this is seldom the case.\n",
    "\n",
    "It is much more efficient to decompose the matrix $A$ into a form that is easier to solve.\n",
    "\n",
    "> There are other reasons to do this for special matrix types and distributed computing which we will discuss later.\n",
    "\n",
    "We have actually already seen this efficiency boost with back-substitution. The equation $U x = b$ solves in $O(n^2)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CThm-lm88mwc"
   },
   "source": [
    "Any square matrix can be decomposed,\n",
    "\n",
    "$A = LU$\n",
    "\n",
    "where:\n",
    "\n",
    "$L$ is a lower triangular matrix\n",
    "\n",
    "$U$ is an upper triangular matrix\n",
    "\n",
    "Now, the linear system becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Ax &= b \\\\\n",
    "LUx &= b\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now let $y = Ux$, such that\n",
    "\n",
    "$$\\begin{align}\n",
    "Ly &= b \\\\\n",
    "Ux &= y\n",
    "\\end{align} $$\n",
    "both of which solve in $O(n^2)$.\n",
    "\n",
    "NOTE: L and U are generally *not unique*.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmuC4gf-vBeD"
   },
   "source": [
    "Example: Return to the previouis example:\n",
    "\n",
    "\\begin{align}\n",
    "4x_1 + 3x_2 - 5x_3 &=& 2 \\\\\n",
    "-2x_1 - 4x_2 + 5x_3 &=& 5 \\\\\n",
    "8x_1 + 8x_2  &=& -3 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Through Gaussian Elimination, we found\n",
    "\n",
    "$$ U=\n",
    "\\begin{bmatrix}\n",
    "4 & 3 & -5 \\\\\n",
    "0 & -2.5 & 2.5 \\\\\n",
    "0 & 0 & 12 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "by clearing the first column by multiplying the first row by $-0.5$ for the second row, and  $2$ for the third. The second column was cleared with the second row multiplied by $-0.8$. These coefficients turn out to be the elemements of the $L$ matrix (with 1's along the diagonal)!\n",
    "\n",
    "$$ L=\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "-0.5 & 1 & 0 \\\\\n",
    "2 & -0.8 & 1 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Let's verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CP0Af5Ztv35r",
    "outputId": "6f01e34e-9b3f-4cf2-9cde-18bd66a919f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original matrix A is:\n",
      " [[ 4  3 -5]\n",
      " [-2 -4  5]\n",
      " [ 8  8  0]] \n",
      "\n",
      "The reconstructed matrix is:\n",
      " [[ 4.  3. -5.]\n",
      " [-2. -4.  5.]\n",
      " [ 8.  8.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# prompt: Do decomposition on the above matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the matrix A\n",
    "A = np.array([[4, 3, -5],\n",
    "              [-2, -4, 5],\n",
    "              [8, 8, 0]])\n",
    "\n",
    "print(\"The original matrix A is:\\n\", A, \"\\n\")\n",
    "L = np.array([[1,0,0],\n",
    "               [-.5, 1,0],\n",
    "               [2,-.8,1]])\n",
    "\n",
    "U = np.array([[4,3,-5],\n",
    "              [0,-2.5,2.5],\n",
    "              [0,0,12]])\n",
    "\n",
    "print(\"The reconstructed matrix is:\\n\", L@U)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4wtwinxR3vd"
   },
   "source": [
    "Let's check the package decomposition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12yze0ctR2vy",
    "outputId": "3f2b4514-efb0-468c-e9ea-87f27af12759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation Matrix (P):\n",
      " [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "Lower Triangular Matrix (L):\n",
      " [[ 1.    0.    0.  ]\n",
      " [-0.25  1.    0.  ]\n",
      " [ 0.5   0.5   1.  ]]\n",
      "Upper Triangular Matrix (U):\n",
      " [[ 8.   8.   0. ]\n",
      " [ 0.  -2.   5. ]\n",
      " [ 0.   0.  -7.5]]\n",
      "\n",
      "Multiply L and U:\n",
      " [[ 8.  8.  0.]\n",
      " [-2. -4.  5.]\n",
      " [ 4.  3. -5.]] \n",
      "which is correct but pivoted!\n",
      "\n",
      "Multiply PLU:\n",
      " [[ 4.  3. -5.]\n",
      " [-2. -4.  5.]\n",
      " [ 8.  8.  0.]] \n",
      "which is the original matrix!\n"
     ]
    }
   ],
   "source": [
    "# Calculate the LU decomposition\n",
    "from scipy.linalg import lu, inv\n",
    "P, L, U = lu(A)\n",
    "\n",
    "print(\"Permutation Matrix (P):\\n\", P)\n",
    "print(\"Lower Triangular Matrix (L):\\n\", L)\n",
    "print(\"Upper Triangular Matrix (U):\\n\", U)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nMultiply L and U:\\n\", L@U, \"\\nwhich is correct but pivoted!\")\n",
    "\n",
    "print(\"\\nMultiply PLU:\\n\", P@L@U, \"\\nwhich is the original matrix!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9Ro4bk8y-C1"
   },
   "source": [
    "NB: $P$ in the above is the permutation matrix that, when multiplied by LU recovers the original matrix. It is *not* the pivoting operation that is done internally (although that matrix is easily obtained!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2P5RjZcxmXRA"
   },
   "source": [
    "# Dr. Mike's Tips!\n",
    "\n",
    "- Direct solver are your 'black box' for most of your needs.\n",
    "- They are the most robust for ill-conditioned systems.\n",
    "- They scale *terribly* (both in system size and parallelization)\n",
    "- If you use them, start with a small system and work upwards.\n",
    "- Generally speaking you won't see a speedup with parallelization until you get a large # of nodes\n",
    "- Warning: Some implementations (numpy) are sophisticated enough to handle singular matricies as well as non-singular (be careful with the answer!)\n",
    "- Sparse matricies are your saving grace! Do your best to protect them (hence store the LU factors, not the inverse!)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOXPvbhaXScY2V34w1nuBQy",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
