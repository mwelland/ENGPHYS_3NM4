{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wW4KU3zHwz10"
   },
   "source": [
    "# Truncation Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlhxbG0Hw4XC"
   },
   "source": [
    "Truncation error occurs when we approximate a mathematical function by truncating an infinite series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XM91306CL5Ne"
   },
   "source": [
    "## Taylor Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zpo33tlL8U2"
   },
   "source": [
    "The Taylor series is a fundamental tool in numerical methods. It allows us to represent a function as an infinite sum of its derivatives at a single point.\n",
    "\n",
    "$f(x+\\Delta x) = f(x) + f'(x) \\Delta x + f''(x) \\frac{\\Delta x^2}{2!} + f'''(x) \\frac{\\Delta x^3}{3!} + \\cdots = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(x)}{n!} \\Delta x^n$\n",
    "\n",
    "However, we cannot compute an infinite number of terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B123-THhPHbb"
   },
   "source": [
    "If $\\Delta x$ is small, the higher-order terms become progressively smaller. This allows us to *truncate* the series after a certain number of terms, creating an approximation.\n",
    "\n",
    "$f(x+\\Delta x) \\approx f(x) + f'(x) \\Delta x + f''(x) \\frac{\\Delta x^2}{2!} + E_k$\n",
    "\n",
    "The error introduced by this truncation is called the **truncation error**, and it is equal to the sum of all the neglected terms:\n",
    "\n",
    "$E_k = \\sum_{n=k}^{\\infty} \\frac{f^{(n)}(x)}{n!} \\Delta x^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0bRhvoVRL5R"
   },
   "source": [
    "The truncation error is often expressed using Big-O notation, which describes the order of the error. If the first neglected term is proportional to $\\Delta x^k$, we say the approximation is of order $k$, or $\\mathcal{O}(\\Delta x^k)$.\n",
    "\n",
    "$f(x_0+\\Delta x) \\approx f(x_0) + f'(x_0) \\Delta x + f''(x_0) \\frac{\\Delta x^2}{2} + \\mathcal{O}(\\Delta x^3)$\n",
    "\n",
    "This notation is useful because it tells us how the error changes as we adjust the step size $\\Delta x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcHnOPUiT6Bw"
   },
   "source": [
    "### Example: Order of the Forward Difference Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfpBqJI5UGL8"
   },
   "source": [
    "Let's find the order of the forward difference approximation for the first derivative:\n",
    "\n",
    "$f'(x) \\approx \\frac{f(x+\\Delta x) - f(x)}{\\Delta x}$\n",
    "\n",
    "We can substitute the Taylor series for $f(x+\\Delta x)$:\n",
    "\n",
    "$f'(x) \\approx \\frac{(f(x) + f'(x) \\Delta x + f''(x) \\frac{\\Delta x^2}{2} + \\cdots) - f(x)}{\\Delta x} = f'(x) + f''(x) \\frac{\\Delta x}{2} + \\cdots$\n",
    "\n",
    "The leading error term is proportional to $\\Delta x$, so the forward difference is a **first-order** approximation: $f'(x) \\approx f'(x) + \\mathcal{O}(\\Delta x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGih8nflWfp-"
   },
   "source": [
    "### Example 2: Order of the Central Difference Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_BxUma_WMFr"
   },
   "source": [
    "Now let's find the order of the central difference approximation:\n",
    "\n",
    "$f'(x) \\approx \\frac{f(x+\\Delta x) - f(x-\\Delta x)}{2 \\Delta x}$\n",
    "\n",
    "Substituting the Taylor series for $f(x+\\Delta x)$ and $f(x-\\Delta x)$:\n",
    "\n",
    "$f'(x) \\approx \\frac{(f(x) + f'(x) \\Delta x + f''(x) \\frac{\\Delta x^2}{2} + \\cdots) - (f(x) - f'(x) \\Delta x + f''(x) \\frac{\\Delta x^2}{2} - \\cdots)}{2 \\Delta x} = f'(x) + f'''(x) \\frac{\\Delta x^2}{6} + \\cdots$\n",
    "\n",
    "The leading error term is proportional to $\\Delta x^2$, so the central difference is a **second-order** approximation: $f'(x) \\approx f'(x) + \\mathcal{O}(\\Delta x^2)$. This means that halving the step size quarters the error, making it much more accurate than the forward difference for the same number of function calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1_GJuO-tpUG"
   },
   "source": [
    "## Common Mathematical Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV094Wz2wmne"
   },
   "source": [
    "Computers are excellent at basic arithmetic, but how do they calculate more complex functions? Often, they use Taylor series expansions.\n",
    "\n",
    "| Function       | Taylor Expansion                                           |\n",
    "|:---------------|:-----------------------------------------------------------|\n",
    "| $\\sin(x)$     | $x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots$ |\n",
    "| $\\cos(x)$     | $1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\cdots$ |\n",
    "| $\\exp(x)$     | $1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\cdots$ |\n",
    "| $\\ln(1+x)$   | $x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4} + \\cdots$   |\n",
    "\n",
    "In infinite precision, these series are globally convergent. However, in finite precision, they can lead to issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-0Boej0mg4e"
   },
   "source": [
    "### Example: Examining the terms of $\\sin(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKxYc2m5o1pC"
   },
   "outputs": [],
   "source": [
    "def taylor_series_sin(x, n_terms):\n",
    "    \"\"\"\n",
    "    Calculates the Taylor series expansion of sin(x) up to n_terms.\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for n in range(n_terms):\n",
    "        term = ((-1)**n * x**(2*n + 1)) / math.factorial(2*n + 1)\n",
    "        terms.append(term)\n",
    "    \n",
    "    print(f\"The terms for sin({x}) are as follows:\")\n",
    "    for i, term in enumerate(terms):\n",
    "        print(f\"Term {i+1}: {term:.10f}\")\n",
    "    \n",
    "    print(f\"Approximate sin({x}): {sum(terms):.10f}\")\n",
    "    print(f\"Actual sin({x}):      {math.sin(x):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymjLGoNrmzxZ"
   },
   "source": [
    "For a small value of x, the terms decrease rapidly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-TNnds0k8qe"
   },
   "outputs": [],
   "source": [
    "taylor_series_sin(1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nak6AIcim9Te"
   },
   "source": [
    "For a large value of x, the terms initially grow very large before converging, which can lead to a loss of precision due to round-off error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1uyHPrTmyq7"
   },
   "outputs": [],
   "source": [
    "taylor_series_sin(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a very large x, the approximation can be completely wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkStK5CRnSOS"
   },
   "outputs": [],
   "source": [
    "taylor_series_sin(100, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oz4jXj5YpqgJ"
   },
   "source": [
    "## Why You Should Use a Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6e5Gk9ApyM2"
   },
   "source": [
    "While the remedy for this specific issue is relatively simple (e.g., using range reduction), it highlights the complexities of implementing numerical functions. In practice, the methods used in packages like NumPy are highly sophisticated, often employing a combination of techniques, including different expansion methods and look-up tables, to ensure both performance and stability.\n",
    "\n",
    "This is why it is almost always better to use a well-tested package rather than implementing these functions yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omaRkDnMqySz"
   },
   "source": [
    "The Taylor expansion is still a valuable tool for understanding the limiting behavior of functions.\n",
    "\n",
    "For small $x$, $\\exp(x) \\approx 1+x$, which is subject to round-off error when $x$ is very close to zero. To address this, packages like NumPy provide specialized functions like `expm1`, which calculates $\\exp(x) - 1$ more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sK8PJHgksbl2"
   },
   "outputs": [],
   "source": [
    "# Poorer approximation for small x\n",
    "print(f\"np.exp(1e-10) - 1 = {np.exp(1e-10) - 1}\")\n",
    "\n",
    "# Better approximation for small x\n",
    "print(f\"np.expm1(1e-10)   = {np.expm1(1e-10)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO0r8d/UNXYqnq6tn/+efDG",
   "collapsed_sections": [
    "XM91306CL5Ne",
    "QcHnOPUiT6Bw",
    "BGih8nflWfp-",
    "P1_GJuO-tpUG"
   ],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
