{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMFG64MHnty9m3DH7ZOA4f5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mwelland/ENGPYHS_3NM4/blob/main/Interpolation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goals\n",
        "- Explore polynomial interpolation of data\n",
        "- See why only low-order polynomials are generally useful (Runge's phenomenon)\n",
        "- Be able to use Legendre and CubicSpline routines"
      ],
      "metadata": {
        "id": "iAiC8-WnsiEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interpolation vs Curve-fitting"
      ],
      "metadata": {
        "id": "qd3E4CZt-UgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpolation and curve-fitting both deal with fitting lists curves to a list of distrete points but there are some key differences in terminology:\n",
        "\n",
        "**Interpolation** seeks a curve that\n",
        "* Goes through all the points in the inputs.\n",
        "* Assumes there is no measurement error in data points\n",
        "* No ambiguity in mapping x and y (no duplicate y's for a given x)\n",
        "* Often used to capture the *local* behaviour\n",
        "\n",
        "**Curve fitting** seeks a curve that\n",
        "* is the *best fit* for all datapoints (in some sense)\n",
        "* doesn't necessarily traverse all the datapoints\n",
        "* permits ambiguity in x-y pairs\n",
        "* Is more of a *global* encapsulation of the data.\n",
        "* generally recovers interpolation as a 'perfect fit' under the interpolation criteria.  \n"
      ],
      "metadata": {
        "id": "DfFJMlBjCYa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpolation"
      ],
      "metadata": {
        "id": "mf3kSBKnUW5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpolation is a more fundamental concept since it was historically *easier* to do, either direclty as a local process and/or adding new information as it was obtained.   "
      ],
      "metadata": {
        "id": "mOMsNSv-UZ2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to consider a few things for interpolation:\n",
        "* Speed of building the model\n",
        "* Speed of adding new data to the model\n",
        "* Speed of execution for interpolated values\n",
        "* Generalizability to N-D"
      ],
      "metadata": {
        "id": "fYbtm4J5UGbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The methods discussed here rely on a fundamental property of linear algebra:\n",
        "**It is always possible to construct a *unique* polynomial of degree $n$ that passes through $n + 1$ distinct data points!**"
      ],
      "metadata": {
        "id": "hpU1wR06VNhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example: Interpolating a Gaussian curve"
      ],
      "metadata": {
        "id": "DfnJu3KJVgeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For illustrative purposes, let's design a toy problem for exploration:"
      ],
      "metadata": {
        "id": "FdSOq_gi_L2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Plot the function exp(-(x/2)^2) from -5 to 5. Then sample 11 times at 1 intervals,  marking the points on the plot and outputting the results as x_d and y_d\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the function\n",
        "def f(x):\n",
        "  return np.exp(-(x/2)**2)\n",
        "\n",
        "# Create x values for plotting\n",
        "x_toy = np.linspace(-6, 6, 100)\n",
        "y_toy = f(x_toy)\n",
        "\n",
        "# Sample 11 times at 1-interval intervals\n",
        "x_d = np.arange(-5, 6, 1)\n",
        "y_d = f(x_d)\n",
        "\n",
        "# Plot the function and sampled points\n",
        "plt.plot(x_toy, y_toy, label='exp(-(x/2)^2)')\n",
        "plt.scatter(x_d, y_d, color='red', label='Sampled points')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Function and Sampled Points')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"x_d:\", x_d)\n",
        "print(\"y_d:\", y_d)\n"
      ],
      "metadata": {
        "id": "NOZPlsrs-nhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal is to use the sampled data (the red points) and recover the 'true' function (in blue) as faithfully as possible."
      ],
      "metadata": {
        "id": "FFOaTV_eVoeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lagrange Polynomial Interpolation"
      ],
      "metadata": {
        "id": "slTqEzxZ_AIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Legendre polynomial interpolation constructs the Legendre polynomial as,\n",
        "$$\n",
        "y(x) = \\sum_{i = 1}^n y_i P_i(x)\n",
        "$$\n",
        "\n",
        "which is a weighted sum of the Lagrange basis polynomials, $P_i(x)$,\n",
        "\n",
        "$$\n",
        "P_i(x) = \\prod_{j = 1, j\\ne i}^n\\frac{x - x_j}{x_i - x_j}.\n",
        "$$\n",
        "\n",
        "N.B.: $\\prod$ means *the product of*, like $\\sum$ means *the sum of*."
      ],
      "metadata": {
        "id": "EtVzyjWbBlJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lagrange basis polynomials\n",
        "\n",
        "By construction,\n",
        "- $P_i(x_j) = 1$ when $i = j$\n",
        "- $P_i(x_j) = 0$ when $i \\ne j$.\n"
      ],
      "metadata": {
        "id": "R-M4PXqDDDmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example: Find and plot the Lagrange basis polynomials"
      ],
      "metadata": {
        "id": "V4ml-IUgDiE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the data:\n",
        "*x = [0, .5, 2]*\n",
        "*y = [1, 3, 2]*\n",
        "\n",
        "\\begin{eqnarray*}\n",
        "P_1(x) &=& \\frac{(x - x_2)(x - x_3)}{(x_1-x_2)(x_1-x_3)} = \\frac{(x - 1)(x - 2)}{(0-1)(0-2)} = \\frac{1}{2}(x^2 - 3x + 2),\\\\\n",
        "P_2(x) &=& \\frac{(x - x_1)(x - x_3)}{(x_2-x_1)(x_2-x_3)} = \\frac{(x - 0)(x - 2)}{(1-0)(1-2)} = -x^2 + 2x,\\\\\n",
        "P_3(x) &=& \\frac{(x - x_1)(x - x_2)}{(x_3-x_1)(x_3-x_2)} = \\frac{(x - 0)(x - 1)}{(2-0)(2-1)} = \\frac{1}{2}(x^2 - x).\n",
        "\\end{eqnarray*}\n",
        "\n",
        "\n",
        "Plot each polynomial and verify the property that $P_i(x_j) = 1$ when $i = j$ and $P_i(x_j) = 0$ when $i \\ne j$."
      ],
      "metadata": {
        "id": "kAJVgucHWb0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: show me the legendre basis polynomials for the data aove using the numpy.polynomial.legendre Legendre\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.polynomial import legendre\n",
        "\n",
        "# Data points\n",
        "x = [0, .5, 2]\n",
        "y = [1, 3, 2]\n",
        "\n",
        "# Calculate the Lagrange basis polynomials\n",
        "n = len(x)\n",
        "P = []\n",
        "for i in range(n):\n",
        "  numerator = 1\n",
        "  denominator = 1\n",
        "  for j in range(n):\n",
        "    if i != j:\n",
        "      numerator = np.polymul(numerator, np.poly1d([1, -x[j]]))\n",
        "      denominator = denominator * (x[i] - x[j])\n",
        "  P.append(np.poly1d(np.polydiv(numerator, denominator)[0]))\n",
        "\n",
        "\n",
        "# Plot the Lagrange basis polynomials\n",
        "x_plot = np.linspace(-1, 3, 100)\n",
        "\n",
        "for i in range(n):\n",
        "    y_plot = P[i](x_plot)\n",
        "    plt.plot(x_plot, y_plot, label=f'P_{i+1}(x)')\n",
        "\n",
        "plt.scatter(x, [1] * len(x), color='black')\n",
        "plt.scatter(x, [0] * len(x), color='red')\n",
        "plt.scatter(x, [0] * len(x), color='red')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('P_i(x)')\n",
        "plt.title('Lagrange Basis Polynomials')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AG2yGm6rFDXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Assembling the polynomial\n",
        "\n",
        "Since $P_{i\\ne j}=0$, and $P_{i = j}=1$, it is trivial to see that for $ y(x) = \\sum_{i = 1}^n \\omega_i P_i(x) $, the coefficients are simply:\n",
        "\n",
        "$$\n",
        "y(x) = \\sum_{i = 1}^n y_i P_i(x)\n",
        "$$"
      ],
      "metadata": {
        "id": "qxrDwzNX4k2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Plot the legendre polynomial from the basis above\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Construct the Lagrange polynomial\n",
        "L = np.poly1d(0)\n",
        "for i in range(n):\n",
        "  L = L + y[i] * P[i]\n",
        "\n",
        "# Plot the Lagrange polynomial\n",
        "y_plot = L(x_plot)\n",
        "plt.plot(x_plot, y_plot, label='L(x)')\n",
        "plt.scatter(x, y, color='red', label='Data points')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('L(x)')\n",
        "plt.title('Lagrange Polynomial Interpolation')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Eo44VWmZBiF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Analysis"
      ],
      "metadata": {
        "id": "7tcVWr0-LjkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe some notes:\n",
        "* For $n$ data points we necessarily produce a unique polynominal that crosses each one.\n",
        "* If we have two measurements at the same input, $x_i = x_j$, $P_i =\\sim \\frac{1}{0}$ which is undefined *unless* $x_i=x_j$ and $y_i=y_j$ in which case the data pair is redundant and can be removed.\n",
        "* Each evalulation of $P(x)$ involves $n-1$ products, and $L(x)$ is the sum of $n$ bases, therefore evaluation is $O(n^2)$\n",
        "* Adding new data means restarting the compuation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BdzS_18s67sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Use sympy to fit a lagrange polynomial to the data above (with some extra points)\n",
        "\n",
        "import sympy as sp\n",
        "\n",
        "x = [0, 1, 2, 3, 4]\n",
        "y = [1, 3, 2, 5, 7]\n",
        "\n",
        "n = len(x)\n",
        "x_sym = sp.Symbol('x')\n",
        "\n",
        "L = 0\n",
        "for i in range(n):\n",
        "    term = y[i]\n",
        "    for j in range(n):\n",
        "        if i != j:\n",
        "            term *= (x_sym - x[j]) / (x[i] - x[j])\n",
        "    L += term\n",
        "\n",
        "\n",
        "print(L)\n",
        "\n",
        "print('which is an ugly way of writing out:')\n",
        "print(L.simplify())\n"
      ],
      "metadata": {
        "id": "iO4XF3G9_vty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Error"
      ],
      "metadata": {
        "id": "y_zevuoSCkfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be shown that the error in the interpolation is,\n",
        "\n",
        "$$\n",
        "y^{true}(x)-y(x) = \\frac{[x-x_1][x-x_2][x-x_3]...[x-x_n]}{(n+1)!} f^{(n+1)}(\\xi)\n",
        "$$\n",
        "\n",
        "where $\\xi$ is in the interval $(x_0, x_n)$.\n",
        "\n",
        "Since for $n$ datapoints there is a unique polynomial of degree $n-1$, which can be expressed as a Lagrange polynomial, **this analysis is universal to all polynomial interpolations!**. The main takeaway is that:\n",
        "\n",
        "*The further a data point is from $x$, the more it contributes to the error.*"
      ],
      "metadata": {
        "id": "3O8KfrhZXNuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Barycentric Lagrange Interpolation"
      ],
      "metadata": {
        "id": "qsqlp74eQmRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to improve the performance of Lagrange Interpolation. Let:\n",
        "\n",
        "$\n",
        "\\Omega(x) = \\prod_{j = 1}^n [x - x_j]\n",
        "$\n",
        "\n",
        "and the *barycentric weights*, $w_i$:\n",
        "\n",
        "$$\n",
        "w_i = \\prod_{j = 1, j\\ne i}^n\\frac{1}{x_i - x_j}.\n",
        "$$\n",
        "\n",
        "and write:\n",
        "\n",
        "$$\n",
        "P_i(x) = \\Omega(x) \\frac{w_i}{x - x_j}.\n",
        "$$\n",
        "\n",
        "and factor the $\\Omega$ out of the sum:\n",
        "\n",
        "$$\n",
        "y(x) = \\Omega(x) \\sum_{j = 1}^n \\frac{w_i}{x - x_j} y_i.\n",
        "$$\n",
        "\n",
        "which is $O(n)$ for evaluation. Calculation of $w_i$ can be formulated recursively, such that each $w_i$ takes $O(n)$ and the full takes $O(n^2)$ with updates n.\n",
        "\n",
        "NB: The weights depend only on $x_i$, not $y_i$ - this means if we are measuring multiple functions on the same spacing, we can reuse the weights, leading to substantial computaitonal savings.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "280TO48vQzJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The benefit being that the calucation of the $\\omega_i$, $O(n^2)$ is precomputed."
      ],
      "metadata": {
        "id": "9P2UZrlQxBLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Barycentric formula\n",
        "\n",
        "We can write one more form which is commonly implemented. Let's add one more piece of data:\n",
        "\n",
        "$$ 1 = \\sum_{j=0}^n P_j = \\Omega(x) \\sum_{j=0}^n \\frac{w_j}{x-x_j}$$\n",
        "\n",
        "then we divide the previous function and write:\n",
        "\n",
        "$$\n",
        "y(x) = \\frac{\\sum_{j = 0}^n \\frac{w_i}{x - x_j} y_i}{\\sum_{j = 0}^n \\frac{w_i}{x - x_j}}\n",
        "$$\n",
        "\n",
        "where we have cancelled $\\Omega$! Besides elegance, his avoids an issue when evaluating $x\\rightarrow x_i$ where roundoff can cause subtractive cancellation. Since the term appears in the numerator and denominator this cancels out!"
      ],
      "metadata": {
        "id": "rBicuMvTzeoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Newton's polynomial"
      ],
      "metadata": {
        "id": "No6Van_v-yhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Newton's polynomial interpolation has the form:\n",
        "\n",
        "$$ y(x) = a_0 + a_1[x-x_0] + a_2 [x-x_0][x-x_1] + \\dots + a_n[x-x_0][x-x_1]\\dots[x-x_n]$$\n",
        "\n",
        "which has the advantage of $O(n)$ evaluations due to recursion and nested multiplication. E.g. for 4 terms,\n",
        "\n",
        "$$ y(x) = a_0 + [x-x_0] \\bigg[a_1  + [x-x_1] \\big[a_2  + [x-x_2] a_3 \\big] \\bigg] $$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pwJCV8dpA77F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Newton's method is also known as the **divided differences**\n",
        "\n",
        "> This was the algorithm was used to calculate function tables like logarithms and trignometry functions. It was then the basis for the *difference engine*, an early mechanical calculator.\n",
        "\n",
        "Let's pick a data point to start at. Say $y(x_0) = a_0 = y_0$,\n",
        "$$a_0 = y_0$$\n",
        "\n",
        "Add the next data point: $y(x_1) = a_0 + a_1(x_1-x_0) = y_1$, or:\n",
        "\n",
        "$$a_1 = \\frac{y_1 - y_0}{x_1 - x_0}$$\n",
        "\n",
        "Now, insert data point $(x_2, y_2)$,\n",
        "\n",
        "$$a_2 = \\frac{\\frac{y_2 - y_1}{x_2 - x_1} - \\frac{y_1 - y_0}{x_1 - x_0}}{x_2 - x_0}$$\n",
        "\n",
        "and similarly,\n",
        "\n",
        "$$a_3 = \\frac{\\frac{\\frac{y_3-y_2}{x_3-x_2} - \\frac{y_2 - y_1}{x_2-x_1}}{x_3 - x_1} - \\frac{\\frac{y_2-y_1}{x_2-x_1}-\\frac{y_1 - y_0}{x_1 - x_0}}{x_2-x_0}}{x_3 - x_0}$$\n",
        "\n",
        "Notice the recurrsion and the division of the differences."
      ],
      "metadata": {
        "id": "0-4bVp6ffw_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generalize this. Define the two-argument function:\n",
        "\n",
        "$$ y[x_1, x_0] = \\frac{y_1 - y_0}{x_1 - x_0}$$\n",
        "\n",
        "and the ternary recursively:\n",
        "\n",
        "$$ y[x_2, x_1, x_0] = \\frac{\\frac{y_2 - y_1}{x_2 - x_1} - \\frac{y_1 - y_0}{x_1 - x_0}}{x_2 - x_0} = \\frac{y[x_2,x_1] - y[x_1,x_0]}{x_2-x_1}$$\n",
        "\n",
        "The $n-nary$ function is:\n",
        "\n",
        "$$ y[x_k, x_{k-1}, \\dots, x_{1}, x_0] = \\frac{y[x_k, x_{k-1}, \\dots, x_{2}, x_2] - y[x_{k-1}, x_{k-2}, \\dots, x_{1}, x_0]}{x_k-x_0}$$"
      ],
      "metadata": {
        "id": "Em398qQ9iQPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize this is in a *tableau*:\n",
        "$$\n",
        "\\begin{array}{cccccc}\n",
        "x_0 & y_0 \\\\\n",
        "    &     & y[x_1,x_0] \\\\\n",
        "x_1 & y_1 &             & y[x_2, x_1,x_0]\\\\\n",
        "    &     & y[x_2,x_1]  &              & y[x_3, x_2, x_1,x_0]\\\\\n",
        "x_2 & y_2 &             & y[x_3, x_2,x_1] &             & y[x_4, x_3, x_2, x_1,x_0]\\\\\n",
        "    &     & y[x_3,x_2]  &              & y[x_4, x_3, x_2, x_1]\\\\\n",
        "x_3 & y_3 &             & y[x_4, x_3,x_2]\\\\\n",
        "    &     & y[x_4,x_3] \\\\\n",
        "x_4 & y_4\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "where element is the difference of the two to the left.\n",
        "Alternately, it is sometimes written in the form,  \n",
        "\n",
        "$$\n",
        "\\begin{array}{c||cccccc}\n",
        "x_0 & y_0 & 0 & 0 & 0 & 0\\\\\n",
        "x_1 & y_1 & y[x_1,x_0] & 0 & 0 & 0\\\\\n",
        "x_2 & y_2 & y[x_2,x_1] & y[x_2, x_1,x_0] & 0          & 0 \\\\\n",
        "x_3 & y_3 & y[x_3,x_2] & y[x_3, x_2,x_1] & y[x_3, x_2, x_1,x_0] & 0            \\\\\n",
        "x_4 & y_4 & y[x_4,x_3] & y[x_4, x_3,x_2] & y[x_4, x_3, x_2, x_1]  & y[x_4, x_3, x_2, x_1,x_0]  \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Note that the diagonal is the coefficients that we need, i.e. $a_0, a_1, a_2, a_3, a_4$ for the polynomial."
      ],
      "metadata": {
        "id": "Ld5E0VgzjONV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Analysis"
      ],
      "metadata": {
        "id": "GfcZ-kFmm_Qr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The order that the datapoints are added is arbitrary but will result in a different tableau (with the same diagonal).\n",
        "* We can build this matrix / tableau diagonal-by-diagonal which means adding new data points doesn't require recalculation of the others.\n",
        "* Each new diagonal (datapoint) takes $O(n)$ so assembly of the tableau takes $~O(n^2)$.\n",
        "* Evaluation of f(x) takes $O(n)$\n",
        "* These coefficients are independant of $x$\n"
      ],
      "metadata": {
        "id": "3S6MU-GWnsgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Direct solution"
      ],
      "metadata": {
        "id": "YGR_0VmHn-kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, there is a direct solution method that only became really poractical with the advent of modern computing since it focusses on linear systes:\n",
        "\n",
        "Consider fitting a function\n",
        "\n",
        "$$ y(x) = a_n x^n + a_{n-1} x^{n-1} \\dots a_2 x^2 + a_1 x +a_0$$\n",
        "\n",
        "since\n",
        "\n",
        "$y(x_i) = a_n x_i^n + a_{n-1} x_i^{n-1} \\dots a_2 x_i^2 + a_1 x_i +a_0 = y_i$\n",
        "\n",
        "we can write out in matrix form,\n",
        "\n",
        "$$\n",
        " \\begin{bmatrix}\n",
        "1 & x_1 & x_1^2 & \\cdots & x_1^m \\\\\n",
        "1 & x_2 & x_2^2 & \\cdots & x_2^m \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_n & x_n^2 & \\cdots & x_n^m\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "a_0 \\\\\n",
        "a_1 \\\\\n",
        "a_2 \\\\\n",
        "\\vdots \\\\\n",
        "a_m\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "y_3 \\\\\n",
        "\\vdots \\\\\n",
        "y_n\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "where the matrix of coefficients is called a Vandermonde matrix. This system can be solved for $a_i$ with a dense linear solver. The issue with this method is that the system is notoriously ill-conditioned and roundoff error accumulates rapidly for large $n$."
      ],
      "metadata": {
        "id": "O40w5_uAoHEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example: Interpolate our toy problem"
      ],
      "metadata": {
        "id": "d5_Mc-Mu_iAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us know examine our toy problem. Since all the polynomial interpolation functions generate the same unique polynomial, any will suffice:"
      ],
      "metadata": {
        "id": "6qbtOfAdpC1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Interpolate the data in x_d and y_d using numpy Legendre, and plot along with the original curve from -5.5 to 5.5\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Interpolate using numpy Legendre\n",
        "coefficients = legendre.legfit(x_d, y_d, len(x_d) - 1)\n",
        "legendre_polynomial = legendre.Legendre(coefficients)\n",
        "\n",
        "# Create x values for plotting the interpolated polynomial\n",
        "x_interp = np.linspace(-5.5, 5.5, 200)\n",
        "y_interp = legendre_polynomial(x_interp)\n",
        "\n",
        "\n",
        "# Plot the original curve, sampled points, and interpolated polynomial\n",
        "plt.plot(x_toy, y_toy, label='exp(-(x/2)^2)')\n",
        "plt.scatter(x_d, y_d, color='red', label='Sampled points')\n",
        "plt.plot(x_interp, y_interp, label='Legendre Interpolation')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Function, Sampled Points, and Legendre Interpolation')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4kY_DJP3_psq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YIKES!\n",
        "\n",
        "This is an example of *Runge's phenomenon*: That even for a seeminlgly ideal case of equalspaced samples, higher order polynomials can show huge oscillations between samples!"
      ],
      "metadata": {
        "id": "x6dKBA1iBEoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cubic splines\n",
        "\n"
      ],
      "metadata": {
        "id": "e5Gjf2e62fbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splines were formulated to relieve these oscillations by piecing together a series of lower-order polynomials and requiring *smoothness*. Consider a polynomial over the interval between $x_i$ and $x_{i+1}$, and assert:\n",
        "* $y(x_i) = y_i$\n",
        "* $y(x_{i+1}) = y_{i+1}$\n",
        "* $y'(x_i)$ be continuous\n",
        "* $y''(x_i)$ be continuous\n",
        "\n",
        "with these 4 constraints, it is clear we are looking for cubic functions, and therefore these splines are *piecewise cubic curves*."
      ],
      "metadata": {
        "id": "tYMRVP6l2tSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be describing the splines in terms of the *knots*, $k_i$ which parameterize the curves. For these splines, these knots are the second derivatives at a point $x_i$.\n",
        "\n",
        "To find the coefficients of the cubic splines, consider that the second derivative is linear and represent it with a 2-point Lagrange interpolation:\n",
        "\\begin{align}\n",
        "y''_{i, i+1} &= k_i P_i(x) + k_{i+1} P_{i+1}(x) \\\\\n",
        "&= \\frac{k_i [x-x_{i+1}] + k_{i+1} [x-x_i]}{x_i-x_{i+1}}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "SG7Nz-zg7GfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the constraints above we end up with:\n",
        "\n",
        "$$k_{i-1}[x_{i-1}-x_i]  + 2 k_i [x_{i-1} - x_{i+1}] + k_{i+1}[x_i-x_{i+1}] = 6\\left[ \\frac{y_{i-1}-y_i}{x_{i-1}-x_i} - \\frac{y_{i}-y_{i+1}}{x_{i}-x_{i+1}} \\right]$$"
      ],
      "metadata": {
        "id": "FAMlSpNG8Q4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "which is a tridiagonal matrix!"
      ],
      "metadata": {
        "id": "waWvtO1f9MpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "1 & 4 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 1 & 4 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 1 & 4 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 1 & 4 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 1 & 4 & 1 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 1 & 4 & 1 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 1 & 4 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 4 & 1 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 4 & 1 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "k_1 \\\\\n",
        "k_2\\\\\n",
        "k_3\\\\\n",
        "k_4\\\\\n",
        "k_5\\\\\n",
        "k_6\\\\\n",
        "k_7\\\\\n",
        "k_8\\\\\n",
        "k_9\\\\\n",
        "k_{10}\\\\\n",
        "k_{11}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0\\\\\n",
        "-0.424 \\\\\n",
        "-1.052 \\\\\n",
        "-0.891 \\\\\n",
        "1.138 \\\\\n",
        "2.654 \\\\\n",
        "1.138 \\\\\n",
        "-0.891 \\\\\n",
        "-1.052 \\\\\n",
        "-0.424 \\\\\n",
        "0\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "EBFNMK6GDNxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Do a cubic spline of x_d and y_d and plot against the original function from -5.5 to 5.5\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.interpolate import CubicSpline\n",
        "\n",
        "# Create a cubic spline interpolation\n",
        "cs = CubicSpline(x_d, y_d)\n",
        "\n",
        "# Create x values for plotting the interpolated spline\n",
        "x_interp = np.linspace(-6, 6, 200)\n",
        "y_interp = cs(x_interp)\n",
        "\n",
        "# Plot the original curve, sampled points, and interpolated spline\n",
        "plt.plot(x_toy, y_toy, label='exp(-(x/2)^2)')\n",
        "plt.scatter(x_d, y_d, color='red', label='Sampled points')\n",
        "plt.plot(x_interp, y_interp, label='Cubic Spline Interpolation')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Function, Sampled Points, and Cubic Spline Interpolation')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EG-kW1NuB-T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Analysis of cubic splines\n"
      ],
      "metadata": {
        "id": "BJKlGjVUgjRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We note:\n",
        "* Cubic splines are *stiffer* in that they don't have high-frequency oscillations (thus avoiding Runge's phenomenon).\n",
        "* The concept of *smoothness* is easy in 1D, but what does it mean for 2D+? How would you ensure continuity along an edge?\n",
        "* Specifying *smoothness* as part of the goals going in suggest this is more of a global scheme. This requires simultaneous linears systems to be solved.\n"
      ],
      "metadata": {
        "id": "G0PE_Emjgmzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generalization to N-dimensions"
      ],
      "metadata": {
        "id": "y_OCs7cPftLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's recap and generalize:\n",
        "* For any $n$ points there is a polynomial that fits it, but because of Runge's phenomenon you don't want to use that!\n",
        "* Piecewise polynomials are *stiffer* and avoids Runge's phenomenon, but smoothness causes issues for N-D"
      ],
      "metadata": {
        "id": "HwKaKNM8fxRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So what do we do? Standard pacakges offer simplistic but pragmatic interpolators (optimized for either rectangular or irregular grids) :\n",
        "* Nearest ND interpolator: Find the nearest data point and use that.\n",
        "* Linear ND interpolators: For each input, a triangulation finds the nearest data points and a linear barycentric Lagrange interpolation is performed.\n",
        "\n",
        "Neither of these are completely satisfactory, so we will have to respost to more advanced methods.\n"
      ],
      "metadata": {
        "id": "Kh0oMQGyhYxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Radial Basis Functions"
      ],
      "metadata": {
        "id": "5btqM9XmBroA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Radial basis functions are an n-dimensional interpolation technique that doesn't rely on polynomials. Rather, we define a radial basis function, called a *kernel*, applied to each data point:\n",
        "\n",
        "$$\\varphi_i(||x-x_i||)$$\n",
        "\n",
        "Commonly, we say $\\varphi_i(x=x_i)\\equiv 1$.\n",
        "\n",
        "The kernel only depends on the Euclidian distance between the associated data point, $x_i$ and the evaluation point $x$ (and are therefore *radial*).  \n",
        "\n",
        "The interpolation function $y(x)$ is the weighted sum of the $N$ kernels:\n",
        "\n",
        "$$y(x) = \\sum_i^N \\omega_i \\varphi_i(||x-x_i||)$$"
      ],
      "metadata": {
        "id": "i4Lmmw6iBzqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine the weights $w_i$, we use the data points we have. Consider the $i$'th datapoints,\n",
        "\n",
        "$$y(x_i) = \\sum_j^N \\omega_j \\varphi_j(||x-x_j||)=y_i$$\n",
        "\n",
        "and applied to all N data points generates a linear system:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\phi(\\|x_1 - x_1\\|) & \\phi(\\|x_1 - x_2\\|) & \\cdots & \\phi(\\|x_1 - x_n\\|) \\\\\n",
        "\\phi(\\|x_2 - x_1\\|) & \\phi(\\|x_2 - x_2\\|) & \\cdots & \\phi(\\|x_2 - x_n\\|) \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi(\\|x_n - x_1\\|) & \\phi(\\|x_n - x_2\\|) & \\cdots & \\phi(\\|x_n - x_n\\|)\n",
        "\\end{bmatrix}  \\begin{bmatrix}\n",
        "\\omega_1 \\\\\n",
        "\\omega_2 \\\\\n",
        "\\vdots \\\\\n",
        "\\omega_n\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_n\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "which we know how to solve!"
      ],
      "metadata": {
        "id": "7fPhsmfLB2gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernels are defined with $r = \\| x-x_i\\|$ and a tuning parameter $\\epsilon$. Some common simple kernels are:\n",
        "\n",
        "| Kernel |                   Formula          |\n",
        "|---|-----------------------|\n",
        "| Gaussian |  $e^{-\\epsilon^2 r^2}$ |\n",
        "| Inverse quadratic | $\\frac{1}{1+[\\epsilon r ]^2}$ |\n",
        "| Inverse multiquadric | $\\frac{1}{\\sqrt{1+[\\epsilon r ]^2}}$ |\n",
        "\n",
        "Determination of optimal $\\epsilon$ is a nuanced question, but a good rule of thumb is to use the average distance between samples.\n",
        "\n",
        "$\\epsilon = avg \\|x_i-x_j\\|$\n",
        "\n",
        "\n",
        "Let's see the kernels:"
      ],
      "metadata": {
        "id": "1xe5Qq1UB3X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Plot the above radial basis functions for epsilon = 1\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the radial basis functions\n",
        "def gaussian(r, epsilon):\n",
        "  return np.exp(-(epsilon * r)**2)\n",
        "\n",
        "def inverse_quadratic(r, epsilon):\n",
        "  return 1 / (1 + (epsilon * r)**2)\n",
        "\n",
        "def inverse_multiquadric(r, epsilon):\n",
        "  return 1 / np.sqrt(1 + (epsilon * r)**2)\n",
        "\n",
        "epsilon = 1\n",
        "\n",
        "# Create a range of r values\n",
        "r_values = np.linspace(0, 10, 100)\n",
        "\n",
        "# Calculate the function values for each kernel\n",
        "gaussian_values = gaussian(r_values, epsilon)\n",
        "inverse_quadratic_values = inverse_quadratic(r_values, epsilon)\n",
        "inverse_multiquadric_values = inverse_multiquadric(r_values, epsilon)\n",
        "\n",
        "# Plot the radial basis functions\n",
        "plt.plot(r_values, gaussian_values, label='Gaussian')\n",
        "plt.plot(r_values, inverse_quadratic_values, label='Inverse Quadratic')\n",
        "plt.plot(r_values, inverse_multiquadric_values, label='Inverse Multiquadric')\n",
        "\n",
        "plt.xlabel('r')\n",
        "plt.ylabel('Ï†(r)')\n",
        "plt.title('Radial Basis Functions (epsilon = 1)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t-G9aMCVB7N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> In general, $\\varphi_i(r=0)$ is not necessarily $1$, and $\\varphi(r \\rightarrow \\infty) \\ne 0$, but this requires one more key factor to implement robustly."
      ],
      "metadata": {
        "id": "Fq4lY9ZRB_AG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example - Our Toy problem from last lecture (Gaussian sampled at 10 points, equally spaced)"
      ],
      "metadata": {
        "id": "zBcs5znXCBjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sampled gaussian\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the function\n",
        "def f(x):\n",
        "  return np.exp(-(x/2)**2)\n",
        "\n",
        "def gaussian(r, epsilon):\n",
        "  return np.exp(-(epsilon * r)**2)\n",
        "\n",
        "# Create x values for plotting\n",
        "x_toy = np.linspace(-6, 6, 100)\n",
        "y_toy = f(x_toy)\n",
        "\n",
        "# Sample 11 times at 1-interval intervals\n",
        "x_d = np.arange(-5, 6, 1)\n",
        "y_d = f(x_d)"
      ],
      "metadata": {
        "id": "rIeUd2xZB7sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Construct gaussian radial basis functions and fit to y_d and x_d\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Create a matrix of the radial basis functions\n",
        "phi_matrix = np.zeros((len(x_d), len(x_d)))\n",
        "\n",
        "epsilon = 1\n",
        "\n",
        "for i in range(len(x_d)):\n",
        "  for j in range(len(x_d)):\n",
        "    phi_matrix[i, j] = gaussian(np.abs(x_d[i] - x_d[j]), epsilon)\n",
        "\n",
        "#~~ How do we solve for w_i?\n",
        "# Take a look at the matrix!\n",
        "\n",
        "\n",
        "\n",
        "# #~~ Answer\n",
        "# np.set_printoptions(precision=2, suppress=True)\n",
        "# print(phi_matrix)\n",
        "# weights = np.linalg.solve(phi_matrix, y_d)\n",
        "# #~~\n",
        "\n",
        "# Define the interpolation function\n",
        "def interpolation_function(x, weights, x_d, epsilon):\n",
        "  y = 0\n",
        "  for i in range(len(x_d)):\n",
        "    y += weights[i] * gaussian(np.abs(x - x_d[i]), epsilon)\n",
        "  return y\n",
        "\n",
        "# Interpolate y_fit\n",
        "y_fit = [interpolation_function(x, weights, x_d, epsilon) for x in x_toy]\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(x_toy, y_toy, label='Original Function')\n",
        "plt.scatter(x_d, y_d, color='red', label='Data Points')\n",
        "plt.plot(x_toy, y_fit, label='Interpolation', linestyle='--')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Radial Basis Function Interpolation (Gaussian Kernel)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SUSvxSrWCG1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note this is a great result, but it works because the true function tends to zero outside of the data samples."
      ],
      "metadata": {
        "id": "l38lxYC0DD05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example - 2D gaussian"
      ],
      "metadata": {
        "id": "cnQcr2iRDIQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate a function exp(-x^2-7*y^2)*sin(x)*cos(8y), sample 100 times and fit using gaussian radial basis functions as done above. Plot the original function with the data samples, then the fit.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "\n",
        "# Define the function\n",
        "def f(x, y):\n",
        "  return np.exp(-x**2 - 7*y**2) * np.sin(x) * np.cos(8*y)\n",
        "\n",
        "# Create a grid of x and y values\n",
        "x = np.linspace(-3, 3, 100)\n",
        "y = np.linspace(-3, 3, 100)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = f(X, Y)\n",
        "\n",
        "# Sample 100 times\n",
        "num_samples = 100\n",
        "x_samples = np.random.uniform(-3, 3, num_samples)\n",
        "y_samples = np.random.uniform(-3, 3, num_samples)\n",
        "z_samples = f(x_samples, y_samples)\n",
        "\n",
        "# Plot the original function and data samples\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, alpha=0.5)\n",
        "ax.scatter(x_samples, y_samples, z_samples, color='red', marker='o', s=20)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('z')\n",
        "ax.set_title('Original Function and Data Samples')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Define the radial basis function (Gaussian)\n",
        "def gaussian_2d(x1, y1, x2, y2, epsilon):\n",
        "    r = np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
        "    return np.exp(-(epsilon * r)**2)\n",
        "\n",
        "def phi_matrix_2d(x_samples, y_samples, epsilon):\n",
        "    num_samples = len(x_samples)\n",
        "    phi_matrix = np.zeros((num_samples, num_samples))\n",
        "    for i in range(num_samples):\n",
        "        for j in range(num_samples):\n",
        "            phi_matrix[i, j] = gaussian_2d(x_samples[i], y_samples[i], x_samples[j], y_samples[j], epsilon)\n",
        "    return phi_matrix\n",
        "\n",
        "phi_matrix = phi_matrix_2d(x_samples, y_samples, epsilon = 1)\n",
        "\n",
        "# #~~ Examine the condition number of the matrix before inverting it.\n",
        "# print('The matrix condition number is, ', np.linalg.cond(phi_matrix))\n",
        "# distances = []\n",
        "# for i in range(num_samples):\n",
        "#   for j in range(i + 1, num_samples):\n",
        "#     distance = np.sqrt((x_samples[i] - x_samples[j])**2 + (y_samples[i] - y_samples[j])**2)\n",
        "#     distances.append(distance)\n",
        "# average_distance = np.mean(distances)\n",
        "# eps = average_distance\n",
        "# phi_matrix = phi_matrix_2d(x_samples, y_samples, epsilon= eps)\n",
        "# print('The matrix condition number is, ', np.linalg.cond(phi_matrix))\n",
        "# #~~~\n",
        "\n",
        "# Calculate the weights\n",
        "weights = np.linalg.solve(phi_matrix, z_samples)\n",
        "\n",
        "\n",
        "\n",
        "# Define the interpolation function\n",
        "def interpolation_function_2d(x, y, weights, x_samples, y_samples, epsilon):\n",
        "    z = 0\n",
        "    for i in range(num_samples):\n",
        "        z += weights[i] * gaussian_2d(x, y, x_samples[i], y_samples[i], epsilon)\n",
        "    return z\n",
        "\n",
        "# Interpolate Z_fit\n",
        "Z_fit = np.zeros((100, 100))\n",
        "for i in range(100):\n",
        "    for j in range(100):\n",
        "        Z_fit[i, j] = interpolation_function_2d(x[i], y[j], weights, x_samples, y_samples, epsilon=3)\n",
        "\n",
        "# Plot the fitted function\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X, Y, Z_fit, cmap=cm.coolwarm, alpha=0.5)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('z')\n",
        "ax.set_title('Fitted Function (RBF)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RWtS3JsVDBBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s6JQ8TpxDNVN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}